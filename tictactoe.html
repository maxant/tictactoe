<html>
<head>
    <script>

//https://en.wikipedia.org/wiki/Tic-tac-toe
// 26,830 possible games
// if X starts, chances are 91 wins, 44 losses, 3 draws

//X always starts => that way computer can use those games to learn from. eg user plays and wins.

//try to win instantly: that way it doesnt need to have all possible games in memory
//     but maybe only fall back to that if it cannot win based on memory?

//its ok to use massive memory. why should machines be based on human learning?!

//without the ai strategy, i can win 7/8 times if i start, because it doesnt know how to block starting in the middle or corners

/*
win from middle:
---  -O-  -O-                                                            -O-  -O-
-X-  -X-  -XX  at this stage, O is already being forced and controlled.  OXX  OXX now X has two winning moves: top left or top right. O losses.
---  ---  ---                                                            ---  --X
the way to block winning from the middle is to move in a corner. outcome is always a draw i think.

win from corners
X--  X--  X--
---  ---  --- X now has three ways to win: middle middle or middle left or bottom middle
---  X--  X-X


computer now learns against itself. after 500 moves its not clever enough to avoid losing to central start.
after a number of goes, it learns that nowhere is good for the first move. then it starts learning that the second move is critical.
eventually it will draw, and will learn how to. thats the plan. lets test it!
*/

//TODO add exploration: https://en.wikipedia.org/wiki/Reinforcement_learning => exploration vs exploitation of known outcomes
        //  numTimesExploited: 0,
        //  numTimesExplored: 0,

//TODO dont remove wins. rather cound wins draws losses and use it in the decision of where to go next. if you won more than you lost, it is a good move und umgekehrt.

//TODO is it important how many times they won from the given position using a certain move? probably. esp if not playing random. so we should count how often they won for each move, and use that info together with which is the quickest move, to determine what the next move should be
// => the reward depends not just on the win, but also on the move. can we say that?
//TODO measure percentages after say 1000, 2000, and 5000 moves.  measure with instantWinLoss, without, and without strategy too.
//TODO how many games are required before it learns enough to not require checking instant wins/losses?
//TODO does it develop the strategy to go on the corners first, as your more likely to win? it should
//     do, because it tries winning moves which come from shortest games...
//TODO add different strategies?? add eg neural network?
//TODO remove duplicates in history
//TODO dont play against a random, play against itself! otherwise its not learning properly?

var model = {};

var O = "O";
var X = "X";
var HUMAN = O;
var COMPUTER = X;
var DRAW = "draw";

function init(){
    var canvas = document.getElementById("canvas");

    canvas.width  = window.innerWidth;
    canvas.height = window.innerHeight;

    document.addEventListener("click", click);

    buildModel();

	// ////////////////////
	// configuration
	// ////////////////////
	model.strategy = 'patternRecognition';
	//model.strategy = 'none'; see comment above. without a strategy, the computer doesnt learn to block humans who know about corners/middle
	model.useInstantWinLossChecks = false; //markedly improves intelligence quickly.

    test();
    clear();
    resetStats();

    render();
}

function click(e){

	var cell;

    if(isHit(e, model.clear)){
        clear();
        render();
    }else if(isHit(e, model.resetStats)){
        resetStats();
        render();
    }else if(isHit(e, model.playAutonomously)){
        toggleAuto();
    }else if(isHit(e, model.whoPlaysWhat)){
        toggleWhoPlaysWhat();
    } else if(isNotFinished()){
		//find where user is clicking and move there
        for(i = 0; i < model.board.length; i++){
            for(j = 0; j < model.board[i].length; j++){
                cell = model.board[i][j];
                if(!cell.v && isHit(e, cell)){
                    selectCell(i, j, HUMAN);
                    playAi(COMPUTER, HUMAN);
                    return;
                }
            }
        }
    }
}

function isNotFinished(){
	//the game is still in play if not drawn and there is no winner
	return !model.winner && !model.draw;
}

function isFinished(){
	return !isNotFinished();
}

function toggleWhoPlaysWhat(){
    model.computerStarts = !model.computerStarts;
    clear(); //restart game
    render();
}

/** does a move for the computer if the came is not yet finished */
function playAi(player, rival){
    checkAndHandleFinished();
    if(isNotFinished()){
        doAiMove(player, rival);
        checkAndHandleFinished();
    }
    render();
}

function toggleAuto(){
    model.playAutonomously.active = !model.playAutonomously.active;
    autoPlay();
}

function autoPlay(){
    if(model.playAutonomously.active){
        if(isFinished()){
            clear();
        }
        playAi(HUMAN, COMPUTER);
        playAi(COMPUTER, HUMAN);
        setTimeout(autoPlay, 0);
    }
}

/** is the click event inside the cell? */
function isHit(e, cell){
    return (e.x >= cell.x && e.x <= cell.x + cell.w) &&
            (e.y >= cell.y && e.y <= cell.y + cell.h);
}

//https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/random
function getRandomInt(min, max) {
    min = Math.ceil(min);
    max = Math.floor(max);
    return Math.floor(Math.random() * (max - min)) + min; //The maximum is exclusive and the minimum is inclusive
}

/** this is where the heavy lifting is done. */
function doAiMove(player, rival){

	if(model.useInstantWinLossChecks || (model.playAutonomously.active && player === HUMAN)){ //train the AI against something a little more intelligent so it isnt learning to beat a monkey
		// ////////////////////////////
		// search for an instant win
		// ////////////////////////////
		// copy board
		var copyOfBoard = buildEmtpyBoard();
		for(i = 0; i < model.board.length; i++){
			for(j = 0; j < model.board[i].length; j++){
				copyOfBoard[i][j].v = model.board[i][j].v;
			}
		}
		// attempt all places that are free
		for(i = 0; i < model.board.length; i++){
			for(j = 0; j < model.board[i].length; j++){
				if(!copyOfBoard[i][j].v){
					copyOfBoard[i][j].v = player;
					if(checkFinished(copyOfBoard) === player){
						console.log("found instant win at " + i + "," + j);
						selectCell(i, j, player);
						model.currentPatternKnown = "No, but found an instant win";
						return;
					}else{
						//reset and try next free cell
						delete copyOfBoard[i][j].v;
					}
				}
			}
		}

		// ////////////////////////////
		// avoid instant loss
		// ////////////////////////////
		for(i = 0; i < model.board.length; i++){
			for(j = 0; j < model.board[i].length; j++){
				if(!copyOfBoard[i][j].v){
					copyOfBoard[i][j].v = rival;
					if(checkFinished(copyOfBoard) === rival){
						console.log("found instant lose at " + i + "," + j);
						selectCell(i, j, player);
						model.currentPatternKnown = "No, but avoiding loss";
						return;
					}else{
						//reset and try next free cell
						delete copyOfBoard[i][j].v;
					}
				}
			}
		}

		// ////////////////////////////////////////////////////
		// No instant win, so use the strategy to get a win
		// ////////////////////////////////////////////////////
	}

    model.currentPatternKnown = "Yes";
    if(model.strategy === 'patternRecognition'){
        // have we seen this pattern? if so, and we had a winning solution, repeat it
        var pattern = buildPattern(model.board);
        var memory = model.patternRecognition[pattern];
        var move, possibleMove;
		var placesToMove = [];
		var winRewards, drawRewards, lossRewards;
		var maxReward = 0;
        if(memory){
			console.debug('pattern ' + pattern + ' known');
            //structure:
            //model.patternRecognition[pattern] = {
            //  numTimesExploited: 0,
            //  numTimesExplored: 0,
            //  possibleNextMoves: [{
            //    i: 0,
            //    j: 0,
            //    xWinRewards: 0,
            //    oWinRewards: 0,
            //    drawRewards: 0
            //  }]
            //}

            for(pnm = 0; pnm < memory.possibleNextMoves.length; pnm++){
                possibleMove = memory.possibleNextMoves[pnm];
                if(!model.board[possibleMove.i][possibleMove.j].v){
                    winRewards = player === X ? possibleMove.xWinRewards : possibleMove.oWinRewards;
                    drawRewards = possibleMove.drawRewards;
                    lossRewards = player === X ? possibleMove.oWinRewards : possibleMove.xWinRewards;

					move = {
						winRewards: winRewards,
						drawRewards: drawRewards,
						lossRewards: lossRewards,
						i: possibleMove.i,
						j: possibleMove.j,
					};
					//TODO how to tune this function?
					move.reward = (100*winRewards) + (1*drawRewards) - (100*lossRewards);

					maxReward = Math.max(maxReward, move.reward);

					placesToMove.push(move);

					console.debug('possible known move: ' + JSON.stringify(move));
				}
            }

			//add unknown places with a reward of 0, since its unknown. better to go there than a place which is known to lose.
			var explore = (getRandomInt(0, 100) % (model.playAutonomously.active?2:10)) === 0; //TODO use function? 10% chance of exploring during normal game. otherwise 50% during autoplay
			console.debug('explore: ' + explore + ", placesToMove.length=" + placesToMove.length);
			if(placesToMove.length > 0){
				model.currentPatternKnown = "Yes";
				if(explore){
					model.currentPatternKnown += ", but exploring";
				}
			}else {
				model.currentPatternKnown = "No";
			}
			for(i = 0; i < model.board.length; i++){
				for(j = 0; j < model.board[i].length; j++){
					if(!model.board[i][j].v){
						var found = false;
						for(pnm = 0; pnm < placesToMove.length; pnm++){
							if(placesToMove[pnm].i === i && 
							   placesToMove[pnm].j === j){
							   
							   found = true;
							   break;
						   }
						}
						if(!found){
							placesToMove.push({
								untried: true,
								winRewards: 0,
								drawRewards: 0,
								lossRewards: 0,
								i: i,
								j: j,
								reward: explore ? getRandomInt(maxReward+1, maxReward+1000) : 0 //IF EXPLORING, then use a random reward which is MUCH higher than any other reward
							});
							console.debug('found unknown place to move: ' + JSON.stringify(placesToMove[placesToMove.length-1]));
						}
					}
				}
			}

            placesToMove.sort(function(a, b){
				return a.reward < b.reward;
            });
            console.log("sorted places to move: " + JSON.stringify(placesToMove.map(function(e){return {i: e.i, j: e.j, r: e.reward};})));

			var indexToMove = explore ? getRandomInt(0, placesToMove.length) : 0;
            console.log("using index: " + indexToMove + ". exploring? " + explore);
			var placeToMove = placesToMove[indexToMove];
			if(placeToMove.untried && !explore){
				model.currentPatternKnown += "; never tried, better than losing";
			}

			//TODO different logic: find a square where i have a move that won lots, otherwise a square that i have drawn lots, otherwise a square where i have a move that didnt lose
			// somehow though, im not thinking right. its important that i also lost loads on a square. just because i won once, doesnt mean that the 150 losses are irelevant.
			//so how do we choose a square to go to? probability?
			//from square 0,0 i have won 3 times and lost 100
			//from square 0,1 i have never one, but only lost once
			//i dont really know enough about square 0,1 to say that its better. its less likely that i will lose (1 out of 101 or 1 out of 104 moves)
			//at the same time, its more likely that i will win if i go to square 0,0 because there, three moves out of 104 won, compared to square 0,1 where its zero.
			//exploration is important, as it will help me improve knowledge about untested squares. base exploration on where i have gone least.
			//so how do i decide where to go now, based on the info above???
			//
			//    3/104=2.88% chance of win    |    |
			//    100/104=96.2% chance of loss |    |
			//    ---------------------------------------
			//    0/104=0% win                 |    |
			//    1/104=0.96% loss             |    |
			//    ---------------------------------------
			//                                 |    |
			//
			// so based on percentages based on total moves, we are most likely to lose. so lets avoid that.
			// use exploration to increase wins!
			//

			selectCell(placeToMove.i, placeToMove.j, player);
			return;
        }
	}

    model.currentPatternKnown = "No";

    //TODO use for testing:
    //moveDeterministically(player);
    //console.log("selecting cell deterministically");
    //TODO use for real:
    console.log("selecting cell randomly");
    moveRandomly(player);
}

function buildPattern(board) {
    var pattern = "";
    var v;
    for(i = 0; i < board.length; i++){
        for(j = 0; j < board[i].length; j++){
            v = board[i][j].v;
            if(!v){
                v = "-";
            }
            pattern += v + "|";
        }
    }
    return pattern;
}

function moveRandomly(player) {
    var k = 0;
    while(true){
        k++;
        if(k > 10000){
			//very unlikely to happen
            console.log("failed to move randomly. moving deterministically!");
            if(!moveDeterministically(player)){
				//should only happen if theres a bug
                console.log("unable to move randomly or deterministically");
            }
            break;
        }else{
            var i = getRandomInt(0, model.board.length);
            var j = getRandomInt(0, model.board.length);
            var cell = model.board[i][j];
            if(!cell.v){
                selectCell(i, j, player);
                break;
            }
        }
    }
}

/** does a raster scan and moves in first empty cell */
function moveDeterministically(player) {
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            var cell = model.board[i][j];
            if(!cell.v){
                selectCell(i, j, player);
                return {i: i, j: j};
            }
        }
    }
}

function selectCell(i, j, v) {
    if(model.board[i][j].v) throw new Error("cell " + i + "," + j + " is already selected by " + model.board[i][j].v);
    console.log("selecting cell " + i + "," + j + " for " + v);
    model.board[i][j].v = v;
    model.history.push({i: i, j:j, v: v});
    console.log("=================");
}

/** returns DRAW if all squares are full with no winner, otherwise COMPUTER or HUMAN depending on who won. otherwise undefined. */
function checkFinished(board) {
    var i, j, winner;

	// TODO rewrite this function to be dynamic so that it works with any board size

    function sameNotNullHorizontal(i){
        return board[i][0].v === board[i][1].v &&
               board[i][1].v === board[i][2].v &&
               board[i][0].v;
    }
    for(i = 0; i < board.length; i++){
        if(sameNotNullHorizontal(i)){
            winner = board[i][0].v;
        }
    }

    function sameNotNullVertical(j){
        return board[0][j].v === board[1][j].v &&
               board[1][j].v === board[2][j].v &&
               board[0][j].v;
    }
    for(j = 0; j < board.length; j++){
        if(sameNotNullVertical(j)){
            winner = board[0][j].v;
        }
    }

    if(board[0][0].v &&
        board[0][0].v === board[1][1].v &&
        board[1][1].v === board[2][2].v
    ){
        winner = board[1][1].v;
    }

    if(board[0][2].v &&
        board[0][2].v === board[1][1].v &&
        board[1][1].v === board[2][0].v
    ){
        winner = board[1][1].v;
    }

    if(!winner){
        //check for a draw
        var allFull = true;
        for(i = 0; i < model.board.length; i++){
            for(j = 0; j < model.board[i].length; j++){
                if(!model.board[i][j].v){
                    allFull = false;
                    break;
                }
            }
        }
        if(allFull){
            return DRAW;
        }
    }

    return winner;
}

function checkAndHandleFinished(){
    var winner = checkFinished(model.board);
    if(winner){
        model.totalGames++;
        if(winner === DRAW){
            model.draw = true;
        } else {
            model.winner = winner;
        }
		handleEnd();
    }
}

function handleEnd(){

    //remember all moves for the future
    var result = model.draw ? DRAW : model.winner;
    var recreatedBoard = buildEmtpyBoard();
    var pattern = "";
    var memory, move;
	var uniqueGameKey = "";
    for(h = -1; h < model.history.length; h++){
		if(h === -1){
			//add an empty board at the start, so the AI works out where good starting moves are
		}else{
			move = model.history[h];
			uniqueGameKey += move.i + "" + move.j + "" + move.v + "|";
			recreatedBoard[move.i][move.j].v = move.v;
		}
        pattern = buildPattern(recreatedBoard);
        memory = model.patternRecognition[pattern];
        if(!memory){
            memory = {
                possibleNextMoves: [],
                numTimesExploited: 0,
                numTimesExplored: 0
            };
            model.patternRecognition[pattern] = memory;
        }


        if(model.history[h+1]){
            var pnm = 0;
			for(; pnm < memory.possibleNextMoves.length; pnm++) {
                var possibleNextMove = memory.possibleNextMoves[pnm];
                if(possibleNextMove.i === model.history[h+1].i &&
                    possibleNextMove.j === model.history[h+1].j ){

                    break;
                }
            }

            if(pnm >= memory.possibleNextMoves.length){
                memory.possibleNextMoves.push(
                    {
                        i: model.history[h+1].i,
                        j: model.history[h+1].j,
                        xWinRewards: 0,
                        oWinRewards: 0,
                        drawRewards: 0
                    }
                );
            }
			var reward = getReward(model.history.length);
            if(result === X) {
                memory.possibleNextMoves[pnm].xWinRewards += reward;
            }else if(result === O){
                memory.possibleNextMoves[pnm].oWinRewards += reward;
            }else{
                memory.possibleNextMoves[pnm].drawRewards += reward;
            }
        }
    }

    if(model.winner === X){
        model.stats.xWins++;
    }else if(model.winner === O){
        model.stats.oWins++;
    }else{
        model.stats.draws++;
    }
	
	var ug = model.uniqueGames[uniqueGameKey];
	if(!ug){
		model.uniqueGames[uniqueGameKey] = 0;
	}
	model.uniqueGames[uniqueGameKey] ++;
	
	var totalUniqueGames = 0;
	for(key in model.uniqueGames){
		totalUniqueGames++;
	}
	console.log("Unique games: " + totalUniqueGames);
	model.totalUniqueGames = totalUniqueGames;
}

/**
 * we want to reward MORE for quicker wins/losses
 *
 * shorter games reward more, so that we avoid instant losses and take advantage of instant wins.
 */
function getReward(gameLength){
	//TODO this can also be tuned
	switch(gameLength) {
		case 9:
			return 1;
		case 8:
			return 2;
		case 7:
			return 4;
		case 6:
			return 8;
		default:
			return 16;
	}
}

function buildEmtpyBoard() {
    var board = [];
    for(i = 0; i < 3; i++){
        board[i] = [];
        for(j = 0; j < 3; j++){
            board[i][j] = {
                i: i,
                j: j,
                x: 20 * i,
                y: 20 * j,
                w: 20,
                h: 20
            };
        }
    }
    return board;
}

function test(){
    model.board[0][0].v = X;
    model.board[1][0].v = X;
    model.board[2][0].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][1].v = X;
    model.board[1][1].v = X;
    model.board[2][1].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][2].v = X;
    model.board[1][2].v = X;
    model.board[2][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][0].v = X;
    model.board[0][1].v = X;
    model.board[0][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[1][0].v = X;
    model.board[1][1].v = X;
    model.board[1][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[2][0].v = X;
    model.board[2][1].v = X;
    model.board[2][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][0].v = X;
    model.board[1][1].v = X;
    model.board[2][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][2].v = X;
    model.board[1][1].v = X;
    model.board[2][0].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    // RESET MODEL READY TO PLAY AND LEARN!!!
    model.patternRecognition = {};
    model.totalGames = 0;
	model.uniqueGames = {};
}

function assert(b, message){
    if(!b) {
        var msg = "Failed test";
        if(message) msg += ": " + message;
        throw new Error(msg);
    }
}

function buildModel(){

    model.computerStarts = true;

    //the board, including sizes. the value attribute will contain whether X or O selects.
    var i, j;
    model.board = buildEmtpyBoard();

    //the clear button
    model.clear = {
        x: 10,
        y: 80,
        w: 40,
        h: 20
    };

    //the resetStats button
    model.resetStats = {
        x: 60,
        y: 80,
        w: 80,
        h: 20
    };

    //the playAutonomously button
    model.playAutonomously = {
        x: 150,
        y: 80,
        w: 130,
        h: 20
    };

    //the whoPlaysWhat button
    model.whoPlaysWhat = {
        x: 290,
        y: 80,
        w: 80,
        h: 20
    };

    //a place to store moves of the current game
    model.history = [];

    //a place to store pattern recognition memory
    model.patternRecognition = {};

    model.totalGames = 0;
	model.uniqueGames = {};

    resetStats();
}

function resetStats(){
    model.stats = {
        xWins: 0,
        oWins: 0,
        draws: 0
    };
}

/** reset the game */
function clear(){
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            delete model.board[i][j].v;
        }
    }
    delete model.winner;
    delete model.draw;
    model.history = [];

    if(model.computerStarts){
        COMPUTER = X;
        HUMAN = O;
        playAi(COMPUTER, HUMAN);
    }else{
        COMPUTER = O;
        HUMAN = X;
    }
}

function render(){
    var canvas = document.getElementById("canvas");
    var ctx = canvas.getContext("2d");
    var i, j, cell, x,y, w, h;

    ctx.font = "14px Arial";
    ctx.fillStyle = "black";
    ctx.strokeStyle = "black";

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            cell = model.board[i][j];
            ctx.strokeRect(cell.x, cell.y, cell.w, cell.h);
            if(cell.v){
                ctx.strokeText(cell.v, cell.x + 5, cell.y + 15);
            }
        }
    }

    ctx.strokeRect(model.clear.x, model.clear.y, model.clear.w, model.clear.h);
    ctx.strokeText("clear", model.clear.x + 6, model.clear.y + 14);

    ctx.strokeRect(model.resetStats.x, model.resetStats.y, model.resetStats.w, model.resetStats.h);
    ctx.strokeText("reset stats", model.resetStats.x + 6, model.resetStats.y + 14);

    ctx.strokeRect(model.playAutonomously.x, model.playAutonomously.y, model.playAutonomously.w, model.playAutonomously.h);
    ctx.strokeText("play autonomously", model.playAutonomously.x + 6, model.playAutonomously.y + 14);

    ctx.strokeRect(model.whoPlaysWhat.x, model.whoPlaysWhat.y, model.whoPlaysWhat.w, model.whoPlaysWhat.h);
    ctx.strokeText("swap roles", model.whoPlaysWhat.x + 6, model.whoPlaysWhat.y + 14);

    ctx.strokeStyle = "red";
    if(model.winner){
        ctx.strokeText("WON BY " + model.winner, model.clear.x + 6, model.clear.y + 40);
    }else if(model.draw){
        ctx.strokeText("DRAW", model.clear.x + 6, model.clear.y + 40);
    }

    ctx.strokeStyle = "black";
    var xp = (100*(model.stats.xWins)/(model.stats.draws+model.stats.xWins+model.stats.oWins)).toFixed(2);
    var op = (100*(model.stats.oWins)/(model.stats.draws+model.stats.xWins+model.stats.oWins)).toFixed(2);
    ctx.strokeText("X wins: " + model.stats.xWins + " ( " + xp + "%)",
        model.clear.x + 6, model.clear.y + 60);
    ctx.strokeText("O wins: " + model.stats.oWins + " ( " + op + "%)",
        model.clear.x + 6, model.clear.y + 75);
    ctx.strokeText("Draws: " + model.stats.draws, model.clear.x + 6, model.clear.y + 90);
    ctx.strokeText("Total games played: " + model.totalGames, model.clear.x + 6, model.clear.y + 105);
    ctx.strokeText("Unique games played: " + model.totalUniqueGames, model.clear.x + 6, model.clear.y + 120);
    ctx.strokeText("Pattern known? " + model.currentPatternKnown, model.clear.x + 6, model.clear.y + 135);

    ctx.strokeText("Tic Tac Toe", 100, 20);
    ctx.strokeText("X starts", 100, 35);
    ctx.strokeText("You play '" + HUMAN + "', computer plays '" + COMPUTER + "'.", 100, 50);
    ctx.strokeText("As you play the computer will learn.", 100, 65);
}

    </script>
    <style>
body {
    margin: 0;
    padding: 0;
}
canvas {
    position: absolute;
    overflow: hidden;
    display: block;
}

    </style>
</head>
<body onload="init();">
<canvas id="canvas"></canvas>
</body>
</html>
