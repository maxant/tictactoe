<html>
<head>
    <script>

var model = {};

var O = "O";
var X = "X";
var HUMAN = O;
var COMPUTER = X;
var DRAW = "draw";

function init(){
    var canvas = document.getElementById("canvas");

    canvas.width  = window.innerWidth;
    canvas.height = window.innerHeight;

    document.addEventListener("click", click);

    buildModel();

	// ////////////////////
	// configuration
	// ////////////////////
	model.useInstantWinLossChecks = false; //if true, markedly improves intelligence quickly. but it is cooler to learn this on its own!

    test();
    clear();
    resetStats();

    render();

    learn();
}

function learn(){
    model.isLearning = true;
    toggleWhoPlaysWhat();
    toggleAuto();
}

function click(e){

	var cell;

    if(isHit(e, model.clear)){
        clear();
        render();
    }else if(isHit(e, model.resetStats)){
        resetStats();
        render();
    }else if(isHit(e, model.playAutonomously)){
        toggleAuto();
    }else if(isHit(e, model.useExploring)){
        toggleUseExploring();
        render();
    }else if(isHit(e, model.whoPlaysWhat)){
        toggleWhoPlaysWhat();
    } else if(isNotFinished()){
		//find where user is clicking and move there
        for(i = 0; i < model.board.length; i++){
            for(j = 0; j < model.board[i].length; j++){
                cell = model.board[i][j];
                if(!cell.v && isHit(e, cell)){
                    selectCell(i, j, HUMAN);
                    playAi(COMPUTER, HUMAN);
                    return;
                }
            }
        }
    }
}

function isNotFinished(){
	//the game is still in play if not drawn and there is no winner
	return !model.winner && !model.draw;
}

function isFinished(){
	return !isNotFinished();
}

function toggleWhoPlaysWhat(){
    model.computerStarts = !model.computerStarts;
    clear(); //restart game
    render();
}

/** does a move for the computer if the came is not yet finished */
function playAi(player, rival){
    checkAndHandleFinished();
    if(isNotFinished()){
        doAiMove(player, rival);
        checkAndHandleFinished();
    }
    render();
}

function toggleAuto(){
    model.playAutonomously.active = !model.playAutonomously.active;
    autoPlay();
}

function toggleUseExploring(){
    model.useExploring.active = !model.useExploring.active;
}

function autoPlay(){
    if(model.isLearning && isFinished()){
        if(model.totalGames % 100 === 0) {   // or model.totalUniqueGames > 200???
            toggleWhoPlaysWhat();
        }
        if(model.totalGames >= 2000){
            model.isLearning = false;
            toggleAuto();
            if(COMPUTER === O){
                toggleWhoPlaysWhat();
            }
            toggleUseExploring(); //turn it off, to make it look better for users
            resetStats();
            clear();
            render();
            document.getElementById("learning").style.display = "none";
        }
    }

    if(model.playAutonomously.active){
        if(isFinished()){
            clear();
        }
        playAi(HUMAN, COMPUTER);
        playAi(COMPUTER, HUMAN);
        setTimeout(autoPlay, 0);
    }
}

/** is the click event inside the cell? */
function isHit(e, cell){
    return (e.x >= cell.x && e.x <= cell.x + cell.w) &&
            (e.y >= cell.y && e.y <= cell.y + cell.h);
}

//https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/random
function getRandomInt(min, max) {
    min = Math.ceil(min);
    max = Math.floor(max);
    return Math.floor(Math.random() * (max - min)) + min; //The maximum is exclusive and the minimum is inclusive
}

/** this is where the heavy lifting is done. */
function doAiMove(player, rival){

	if(model.useInstantWinLossChecks){ // || (model.playAutonomously.active && player === HUMAN)){ //train the AI against something a little more intelligent so it isnt learning to beat a monkey
	    if(doInstantWinMove(player, rival)) {
	        return;
	    }
	}

    // ////////////////////////////////////////////////////
    // No instant win, so use the strategy to get a win
    // ////////////////////////////////////////////////////

    model.currentPatternKnown = "Yes";
    if(model.playAutonomously.active && player === HUMAN) {
        //use well known moves to try and win. see if we can teach the AI to use them
        if(playLikeAnt(player, rival)){
            return;
        }
    } else {

        var move, possibleMove;

        // have we seen this pattern? if so, and we had a winning solution, repeat it
        var pattern = buildPattern(model.board);
        console.debug('pattern ' + pattern + ' known');

        // state, aka memory
        let memory = model.patternRecognition[pattern];
        if(memory){

            let explore = decideWhetherToExplore();

            // determine Q i.e. a function based on state and actions
            // placesToMove contains all possible actions and maps those actions to known rewards. the policy can
            // use it to select the "best" move
            let placesToMove = determinePlacesToMove(explore, memory, player);

            // select based on policy
            let placeToMove = getPlaceToMoveBasedOnPolicy(placesToMove, explore);

            if(placeToMove.untried && !explore){
                model.currentPatternKnown += "; never tried, better than losing";
            }

            selectCell(placeToMove.i, placeToMove.j, player);
            return;
        }
	}

    model.currentPatternKnown = "No";

    console.log("selecting cell randomly");
    moveRandomly(player);
}

/** *******************************************************************************************************
 * https://en.wikipedia.org/wiki/Q-learning:
 *  "A policy pi, is a rule that the agent follows in selecting actions, given the state it is in.
 *   When such an action-value function is learned, the optimal policy can be constructed by selecting
 *   the action with the highest value in each state."
 * The highest value is based on the rewards which were learned from previous games
 ******************************************************************************************************* */
function getPlaceToMoveBasedOnPolicy(placesToMove, explore) {
    placesToMove.sort(function(a, b){
        return a.reward < b.reward;
    });
    console.log("sorted places to move: " + JSON.stringify(placesToMove.map(function(e){return {i: e.i, j: e.j, r: e.reward};})));

    //if exploring, all unknown places were given higher rewards than known moves, so they are now at the front
    //of the list. if all places are known, then exploring doesn't really make sense? actually it does, because
    //maybe last time we lost with a certain move, but this time we can win! so that means there is no point
    //in setting the reward of unknown places.
    //otherwise, select all those with the highest reward, and move randomly within that subset
    if(!explore) {
        placesToMove = placesToMove.filter(function(a){
            return a.reward === placesToMove[0].reward;
        });
        console.log("filtered places to move: " + JSON.stringify(placesToMove.map(function(e){return {i: e.i, j: e.j, r: e.reward};})));
    }
    let indexToMove = getRandomInt(0, placesToMove.length);
    console.log("using index: " + indexToMove + ". exploring? " + explore);
    return placesToMove[indexToMove];
}


function decideWhetherToExplore() {

    //TODO tidy up

    //explore based on ratio of total games to total unique games. eg 3 total vs 1 unique: explore 33%.
    //eg 100 total vs 1 unique: explore 1%
    var explore = false;
    if(model.useExploring.active) {
        // always reducing chance of exploration:
        // total games 5, 100% chance of exploration
        // total games 99, 100% chance of exploration
        // total games 101, 50% chance of exploration
        // total games 201, 33% chance of exploration
        // total games 301, 25% chance of exploration
        // total games 1000, 10% chance of exploration
        // total games 10000, 1% chance of exploration
        //var exploreRatio = Math.ceil(model.totalGames/100);
        var exploreRatio = Math.ceil(model.totalUniqueGames/30); //300 unique => 1% exploration
        console.debug("explore ratio: 1:" + exploreRatio);
        explore = getRandomInt(0, exploreRatio > 1000 ? exploreRatio : 2) === 0;
        //explore = (getRandomInt(0, 100) % (10)) === 0; //TODO use function? 10% chance of exploring during normal game. otherwise 50% during autoplay
    }
    console.debug('explore: ' + explore);
    if(explore) {
        model.numExplorations++;
    }

    return explore;
}

/** *******************************************************************************************************
 * placesToMove is effectively the action-value function Q(s, a) which gives the expected utility of a
 * given action a while in a given state s (https://en.wikipedia.org/wiki/Q-learning).
 * so we can use it to find the best action based on previous rewards.
 * since we want to move to the place with the highest reward, we sort the values by reward.
 ******************************************************************************************************** */
function determinePlacesToMove(explore, memory, player) {

    var placesToMove = [];
    var winRewards, drawRewards, lossRewards;

    //structure:
    //memory = {
    //  possibleNextMoves: [{
    //    i: 0,
    //    j: 0,
    //    xWinRewards: 0,
    //    oWinRewards: 0,
    //    drawRewards: 0
    //  }]
    //}

    for(pnm = 0; pnm < memory.possibleNextMoves.length; pnm++){
        possibleMove = memory.possibleNextMoves[pnm];
        if(!model.board[possibleMove.i][possibleMove.j].v){
            winRewards = player === X ? possibleMove.xWinRewards : possibleMove.oWinRewards;
            drawRewards = possibleMove.drawRewards;
            lossRewards = player === X ? possibleMove.oWinRewards : possibleMove.xWinRewards;

            move = {
                winRewards: winRewards,
                drawRewards: drawRewards,
                lossRewards: lossRewards,
                i: possibleMove.i,
                j: possibleMove.j,
            };
            //TODO how to tune this function?
            var total = winRewards + drawRewards + lossRewards;

            //divide by total, because otherwise it seems to learn that one place is GREAT and doesnt give other ones a chance.
            //this way, we work out the win ratio regardless of how often its been played. so its normalised in comparison to the others.
            move.reward = (100*(winRewards/total)) + (10*(drawRewards/total)) + (-1*(lossRewards/total));

            placesToMove.push(move);

            console.debug('possible known move: ' + JSON.stringify(move));
        }
    }

    // add some logging info for later...
    if(placesToMove.length > 0){
        model.currentPatternKnown = "Yes";
        if(explore){
            model.currentPatternKnown += ", but exploring";
        }
    }else {
        model.currentPatternKnown = "No";
    }

    //finally add unknown places with a reward of 0, since its unknown. better to go there than a place which is known to lose.
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            if(!model.board[i][j].v){
                var found = false;
                for(pnm = 0; pnm < placesToMove.length; pnm++){
                    if(placesToMove[pnm].i === i &&
                       placesToMove[pnm].j === j){

                       found = true;
                       break;
                   }
                }
                if(!found){
                    placesToMove.push({
                        untried: true,
                        winRewards: 0,
                        drawRewards: 0,
                        lossRewards: 0,
                        i: i,
                        j: j,
                        reward: 0
                    });
                    console.debug('found unknown place to move: ' + JSON.stringify(placesToMove[placesToMove.length-1]));
                }
            }
        }
    }
    return placesToMove;
}



function doInstantWinMove(player, rival) {
    // ////////////////////////////
    // search for an instant win
    // ////////////////////////////
    // copy board
    var copyOfBoard = buildEmtpyBoard();
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            copyOfBoard[i][j].v = model.board[i][j].v;
        }
    }
    // attempt all places that are free
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            if(!copyOfBoard[i][j].v){
                copyOfBoard[i][j].v = player;
                if(checkFinished(copyOfBoard) === player){
                    console.log("found instant win at " + i + "," + j);
                    selectCell(i, j, player);
                    model.currentPatternKnown = "No, but found an instant win";
                    return true;
                }else{
                    //reset and try next free cell
                    delete copyOfBoard[i][j].v;
                }
            }
        }
    }

    // ////////////////////////////
    // avoid instant loss
    // ////////////////////////////
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            if(!copyOfBoard[i][j].v){
                copyOfBoard[i][j].v = rival;
                if(checkFinished(copyOfBoard) === rival){
                    console.log("found instant lose at " + i + "," + j);
                    selectCell(i, j, player);
                    model.currentPatternKnown = "No, but avoiding loss";
                    return true;
                }else{
                    //reset and try next free cell
                    delete copyOfBoard[i][j].v;
                }
            }
        }
    }
    return false;
}

// ant moves are what the author does to try and win, ie start in the middle
var antMoves = {
    "---|---|---|": {i: 1, j: 1},

    "-O-|-X-|---|": {i: 1, j: 0},
    "---|OX-|---|": {i: 0, j: 1},
    "---|-XO|---|": {i: 0, j: 1},
    "---|-X-|-O-|": {i: 1, j: 0},
};

function playLikeAnt(player, rival) {
    var pattern = buildPattern(model.board);
    var move = antMoves[pattern];
    if(move) {
        model.currentPatternKnown = "Yes, moving like Ant";
        selectCell(move.i, move.j, player);
        return true;
    } else {
        return doInstantWinMove(player, rival);
    }
}

function buildPattern(board) {
    var pattern = "";
    var v;
    for(i = 0; i < board.length; i++){
        for(j = 0; j < board[i].length; j++){
            v = board[i][j].v;
            if(!v){
                v = "-";
            }
            pattern += v;
        }
        pattern += "|";
    }
    return pattern;
}

function moveRandomly(player) {
    var k = 0;
    while(true){
        k++;
        if(k > 10000){
			//very unlikely to happen
            console.log("failed to move randomly. moving deterministically!");
            if(!moveDeterministically(player)){
				//should only happen if theres a bug
                console.log("unable to move randomly or deterministically");
            }
            break;
        }else{
            var i = getRandomInt(0, model.board.length);
            var j = getRandomInt(0, model.board.length);
            var cell = model.board[i][j];
            if(!cell.v){
                selectCell(i, j, player);
                break;
            }
        }
    }
}

/** does a raster scan and moves in first empty cell */
function moveDeterministically(player) {
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            var cell = model.board[i][j];
            if(!cell.v){
                selectCell(i, j, player);
                return {i: i, j: j};
            }
        }
    }
}

function selectCell(i, j, v) {
    if(model.board[i][j].v) throw new Error("cell " + i + "," + j + " is already selected by " + model.board[i][j].v);
    if(v === COMPUTER){
        model.numAiMoves++;
    }
    console.log("selecting cell " + i + "," + j + " for " + v);
    model.board[i][j].v = v;
    model.history.push({i: i, j:j, v: v});
    console.log("=================");
}

/** returns DRAW if all squares are full with no winner, otherwise COMPUTER or HUMAN depending on who won. otherwise undefined. */
function checkFinished(board) {
    var i, j, winner;

	// TODO rewrite this function to be dynamic so that it works with any board size

    function sameNotNullHorizontal(i){
        return board[i][0].v === board[i][1].v &&
               board[i][1].v === board[i][2].v &&
               board[i][0].v;
    }
    for(i = 0; i < board.length; i++){
        if(sameNotNullHorizontal(i)){
            winner = board[i][0].v;
        }
    }

    function sameNotNullVertical(j){
        return board[0][j].v === board[1][j].v &&
               board[1][j].v === board[2][j].v &&
               board[0][j].v;
    }
    for(j = 0; j < board.length; j++){
        if(sameNotNullVertical(j)){
            winner = board[0][j].v;
        }
    }

    if(board[0][0].v &&
        board[0][0].v === board[1][1].v &&
        board[1][1].v === board[2][2].v
    ){
        winner = board[1][1].v;
    }

    if(board[0][2].v &&
        board[0][2].v === board[1][1].v &&
        board[1][1].v === board[2][0].v
    ){
        winner = board[1][1].v;
    }

    if(!winner){
        //check for a draw
        var allFull = true;
        for(i = 0; i < model.board.length; i++){
            for(j = 0; j < model.board[i].length; j++){
                if(!model.board[i][j].v){
                    allFull = false;
                    break;
                }
            }
        }
        if(allFull){
            return DRAW;
        }
    }

    return winner;
}

function checkAndHandleFinished(){
    var winner = checkFinished(model.board);
    if(winner){
        model.totalGames++;
        if(winner === DRAW){
            model.draw = true;
        } else {
            model.winner = winner;
        }
		handleEnd();
    }
}

function handleEnd(){

    //remember all moves for the future
    var result = model.draw ? DRAW : model.winner;
    var recreatedBoard = buildEmtpyBoard();
    var pattern = "";
    var memory, move;
	var uniqueGameKey = "";
    for(h = -1; h < model.history.length; h++){
		if(h === -1){
			//add an empty board at the start, so the AI works out where good starting moves are
		}else{
			move = model.history[h];
			uniqueGameKey += move.i + "" + move.j + "" + move.v + "|";
			recreatedBoard[move.i][move.j].v = move.v;
		}
        pattern = buildPattern(recreatedBoard);
        memory = model.patternRecognition[pattern];
        if(!memory){
            memory = {
                possibleNextMoves: []
            };
            model.patternRecognition[pattern] = memory;
        }


        if(model.history[h+1]){
            var pnm = 0;
			for(; pnm < memory.possibleNextMoves.length; pnm++) {
                var possibleNextMove = memory.possibleNextMoves[pnm];
                if(possibleNextMove.i === model.history[h+1].i &&
                    possibleNextMove.j === model.history[h+1].j ){

                    break;
                }
            }

            if(pnm >= memory.possibleNextMoves.length){
                memory.possibleNextMoves.push(
                    {
                        i: model.history[h+1].i,
                        j: model.history[h+1].j,
                        xWinRewards: 0,
                        oWinRewards: 0,
                        drawRewards: 0
                    }
                );
            }
			var reward = getReward(model.history.length);
            if(result === X) {
                memory.possibleNextMoves[pnm].xWinRewards += reward;
            }else if(result === O){
                memory.possibleNextMoves[pnm].oWinRewards += reward;
            }else{
                memory.possibleNextMoves[pnm].drawRewards += reward;
            }
        }
    }

    if(model.winner === X){
        model.stats.xWins++;
    }else if(model.winner === O){
        model.stats.oWins++;
    }else{
        model.stats.draws++;
    }
	
	var ug = model.uniqueGames[uniqueGameKey];
	if(!ug){
		model.uniqueGames[uniqueGameKey] = 0;
	}
	model.uniqueGames[uniqueGameKey] ++;
	
	var totalUniqueGames = 0;
	for(key in model.uniqueGames){
		totalUniqueGames++;
	}
	console.log("Unique games: " + totalUniqueGames);
	model.totalUniqueGames = totalUniqueGames;
}

/**
 * we want to reward MORE for quicker wins/losses
 *
 * shorter games reward more, so that we avoid instant losses and take advantage of instant wins.
 */
function getReward(gameLength){
	//TODO this can also be tuned
	switch(gameLength) {
		case 9:
			return 1;
		case 8:
			return 2;
		case 7:
			return 4;
		case 6:
			return 8;
		default:
			return 16;
	}
}

function buildEmtpyBoard() {
    var board = [];
    for(i = 0; i < 3; i++){
        board[i] = [];
        for(j = 0; j < 3; j++){
            board[i][j] = {
                i: i,
                j: j,
                x: 20 * i,
                y: 20 * j,
                w: 20,
                h: 20
            };
        }
    }
    return board;
}

function test(){
    model.board[0][0].v = X;
    model.board[1][0].v = X;
    model.board[2][0].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][1].v = X;
    model.board[1][1].v = X;
    model.board[2][1].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][2].v = X;
    model.board[1][2].v = X;
    model.board[2][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][0].v = X;
    model.board[0][1].v = X;
    model.board[0][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[1][0].v = X;
    model.board[1][1].v = X;
    model.board[1][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[2][0].v = X;
    model.board[2][1].v = X;
    model.board[2][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][0].v = X;
    model.board[1][1].v = X;
    model.board[2][2].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

    model.board[0][2].v = X;
    model.board[1][1].v = X;
    model.board[2][0].v = X;
    checkAndHandleFinished();
    assert(model.winner === X);
    clear();

	resetEverything();
}

function assert(b, message){
    if(!b) {
        var msg = "Failed test";
        if(message) msg += ": " + message;
        throw new Error(msg);
    }
}

function resetEverything(){
    //a place to store moves of the current game
    model.history = [];

    //a place to store pattern recognition memory
    model.patternRecognition = {};

    model.totalGames = 0;
	model.uniqueGames = {};
	model.totalUniqueGames = 0;
	model.numExplorations = 0;
    model.numAiMoves = 0;

	resetStats();
}

function buildModel(){

    model.computerStarts = true;

    //the board, including sizes. the value attribute will contain whether X or O selects.
    var i, j;
    model.board = buildEmtpyBoard();

    //the clear button
    model.clear = {
        x: 10,
        y: 80,
        w: 40,
        h: 20
    };

    //the resetStats button
    model.resetStats = {
        x: 60,
        y: 80,
        w: 80,
        h: 20
    };

    //the playAutonomously button
    model.playAutonomously = {
        x: 150,
        y: 80,
        w: 130,
        h: 20
    };

    //the whoPlaysWhat button
    model.whoPlaysWhat = {
        x: 290,
        y: 80,
        w: 80,
        h: 20
    };

    //the useExploring button
    model.useExploring = {
        x: 380,
        y: 80,
        w: 60,
        h: 20,
        active: true
    };

	resetEverything();
}

function resetStats(){
    model.stats = {
        xWins: 0,
        oWins: 0,
        draws: 0
    };
}

/** reset the game */
function clear(){
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            delete model.board[i][j].v;
        }
    }
    delete model.winner;
    delete model.draw;
    model.history = [];

    if(model.computerStarts){
        COMPUTER = X;
        HUMAN = O;
        playAi(COMPUTER, HUMAN);
    }else{
        COMPUTER = O;
        HUMAN = X;
    }
	console.log("======== cleared ============");
}

function render(){
    var canvas = document.getElementById("canvas");
    var ctx = canvas.getContext("2d");
    var i, j, cell, x,y, w, h;

    ctx.font = "14px Arial";
    ctx.fillStyle = "black";
    ctx.strokeStyle = "black";

    ctx.clearRect(0, 0, canvas.width, canvas.height);
    for(i = 0; i < model.board.length; i++){
        for(j = 0; j < model.board[i].length; j++){
            cell = model.board[i][j];
            ctx.strokeRect(cell.x, cell.y, cell.w, cell.h);
            if(cell.v){
                ctx.strokeText(cell.v, cell.x + 5, cell.y + 15);
            }
        }
    }

    ctx.strokeRect(model.clear.x, model.clear.y, model.clear.w, model.clear.h);
    ctx.strokeText("clear", model.clear.x + 6, model.clear.y + 14);

    ctx.strokeRect(model.resetStats.x, model.resetStats.y, model.resetStats.w, model.resetStats.h);
    ctx.strokeText("reset stats", model.resetStats.x + 6, model.resetStats.y + 14);

    ctx.strokeRect(model.playAutonomously.x, model.playAutonomously.y, model.playAutonomously.w, model.playAutonomously.h);
    ctx.strokeText("play autonomously", model.playAutonomously.x + 6, model.playAutonomously.y + 14);

    ctx.strokeRect(model.whoPlaysWhat.x, model.whoPlaysWhat.y, model.whoPlaysWhat.w, model.whoPlaysWhat.h);
    ctx.strokeText("swap roles", model.whoPlaysWhat.x + 6, model.whoPlaysWhat.y + 14);

    ctx.strokeRect(model.useExploring.x, model.useExploring.y, model.useExploring.w, model.useExploring.h);
    ctx.strokeText("explore", model.useExploring.x + 6, model.useExploring.y + 14);

    ctx.strokeStyle = "red";
    if(model.winner){
        ctx.strokeText("WON BY " + model.winner, model.clear.x + 6, model.clear.y + 40);
    }else if(model.draw){
        ctx.strokeText("DRAW", model.clear.x + 6, model.clear.y + 40);
    }

    ctx.strokeStyle = "black";
    var xp = (100*(model.stats.xWins)/(model.stats.draws+model.stats.xWins+model.stats.oWins)).toFixed(2);
    var op = (100*(model.stats.oWins)/(model.stats.draws+model.stats.xWins+model.stats.oWins)).toFixed(2);
    ctx.strokeText("X wins: " + model.stats.xWins + " ( " + xp + "%)",
        model.clear.x + 6, model.clear.y + 60);
    ctx.strokeText("O wins: " + model.stats.oWins + " ( " + op + "%)",
        model.clear.x + 6, model.clear.y + 75);
    ctx.strokeText("Draws: " + model.stats.draws, model.clear.x + 6, model.clear.y + 90);
    ctx.strokeText("Total games played: " + model.totalGames, model.clear.x + 6, model.clear.y + 105);
    ctx.strokeText("Unique games played: " + model.totalUniqueGames, model.clear.x + 6, model.clear.y + 120);
    ctx.strokeText("Number of explorations: " + model.numExplorations + "; exploring enabled: " + model.useExploring.active, model.clear.x + 6, model.clear.y + 135);
    ctx.strokeText("Number of AI moves: " + model.numAiMoves, model.clear.x + 6, model.clear.y + 150);
    ctx.strokeText("Pattern known? " + model.currentPatternKnown, model.clear.x + 6, model.clear.y + 165);

    ctx.strokeText("Tic Tac Toe", 100, 20);
    ctx.strokeText("X starts", 100, 35);
    ctx.strokeText("You play '" + HUMAN + "', computer plays '" + COMPUTER + "'.", 100, 50);
    ctx.strokeText("As you play the computer will learn.", 100, 65);
}






    </script>
    <style>
body {
    margin: 0;
    padding: 0;
}
canvas {
    position: absolute;
    overflow: hidden;
    display: block;
}






    </style>
</head>
<body onload="init();">
<canvas id="canvas"></canvas>
<div id="learning"
     style="position: fixed; width: 100%; height: 100%; top: 0; left: 0; right: 0; bottom: 0; background-color: rgba(0,0,0,0.2); z-index: 2; font-size:32px; text-align: center;">
    <br>
    <br>
    <br>
    learning... please give me a minute...
</div>
</body>
</html>